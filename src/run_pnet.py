# Script to run Marc Glettig's P-NET on prostate germline and/or somatic data
# Author: Gwen Miller
# Generated by copying over and then editing everything in the file: `pnet/notebooks/prostate_metastatic_prediction_example.ipynb`

import os
import sys

####### TODO: figure out module importing 
import pnet_loader
import util
import ReactomeNetwork # this loads
import Pnet # this fails to load bc of how it imports ReactomeNetwork...

# Gwen's scripts
import data_manipulation
import vcf_manipulation
import prostate_data_loaders
import report_and_eval

import wandb
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt


# Importing packages related to model performance
from sklearn.metrics import confusion_matrix # expects true_labels, predicted_labels
from sklearn.metrics import classification_report # expects true_labels, predicted_labels
from sklearn.metrics import roc_auc_score # expects true_labels, predicted_probs

import logging
logging.basicConfig(
            filename='run_pnet.log', 
            encoding='utf-8',
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.INFO,
            datefmt='%Y-%m-%d %H:%M:%S')

logger = logging.getLogger()
logger.setLevel(logging.INFO)


mutations_dict = {"3'Flank": 'Silent',
                  "5'Flank": 'Silent',
                  "5'UTR": 'Silent',
                  "3'UTR": 'Silent',
                  "IGR": 'Silent',
                  "Intron": 'Silent',
                  "lincRNA": 'Silent',
                  "RNA": 'Silent',
                  "Silent": 'Silent',
                  "non_coding_transcript_exon": 'Silent',
                  "upstream_gene": 'Silent',
                  "Splice_Region": 'Silent',
                  "Targeted_Region": 'Silent',
                  'Splice_Site': 'LOF',
                  'Nonsense_Mutation': 'LOF',
                  'Frame_Shift_Del': 'LOF',
                  'Frame_Shift_Ins': 'LOF',
                  'Stop_Codon_Del': 'LOF',
                  'Stop_Codon_Ins': 'LOF',
                  'Nonstop_Mutation': 'LOF',
                  'Start_Codon_Del': 'LOF',
                  'Missense_Mutation': 'Other_nonsynonymous',
                  'In_Frame_Del': 'Other_nonsynonymous',
                  'In_Frame_Ins': 'Other_nonsynonymous',
                  'De_novo_Start_InFrame': 'Other_nonsynonymous',
                  'De_novo_Start_OutOfFrame': 'Other_nonsynonymous',
                  'Start_Codon_Ins': 'Other_nonsynonymous'
                  }


def main():
    wandb.login()
    """# Load each of your data modalities of interest.
    Format should be samples x genes. Set the sample IDs as the index.

    Data modalities:
    1. somatic amp
    1. somatic del
    1. somatic mut
    1. germline mut (subset to a small number of genes).

    Our somatic data has information for many more genes compared to the germline data. We will need to either:
    1. impute zeros for the excluded germline genes, or 
    2. subset the somatic datasets down to the ones that overlap with the germline data.


    We will be subsetting to the ~1060 samples that we have matched somatic and germline data for.

    NOTE: the PNET loader will automatically restrict the input data modalities to overlapping samples and overlapping genes. 
    This means we will need to be carefull with our input germline dataset if we want to keep all the somatic data."""
    
    # TODO: eventually, want this overall structure of hyperparameters and function calls
    USE_ONLY_PAIRED = True
    CONVERT_IDS_TO = "somatic"
    ZERO_IMPUTE_GERMLINE = True
    ZERO_IMPUTE_SOMATIC = False

    logging.debug("Defining paths for somatic data")
    SOMATIC_DATADIR = "../../pnet_germline/data/pnet_database/prostate/processed"
    somatic_mut_f = os.path.join(SOMATIC_DATADIR, "P1000_final_analysis_set_cross_important_only.csv")
    somatic_cnv_f = os.path.join(SOMATIC_DATADIR, "P1000_data_CNA_paper.csv") # TODO: check how PNET implements splitting this into amp vs del
    
    logging.debug("Defining paths for germline data")
    GERMLINE_DATADIR = "../../pnet_germline/data/"
    # germline_vars_f = os.path.join(GERMLINE_DATADIR, "prostate/prostate_germline_vcf_subset_to_germline_tier_1and2_pathogenic_vars_only.txt")
    germline_vars_f = os.path.join(GERMLINE_DATADIR, "prostate/prostate_germline_vcf_subset_to_germline_tier_12_and_somatic_pathogenic_vars_only.txt")

    logging.debug("Defining paths for the sample metadata")
    id_map_f = os.path.join(GERMLINE_DATADIR, "prostate/germline_somatic_id_map_outer_join.csv") # germline_somatic_id_map_f
    sample_metadata_f = os.path.join(GERMLINE_DATADIR,"prostate/pathogenic_variants_with_clinical_annotation_1341_aug2021_correlation.csv")

    somatic_mut = prostate_data_loaders.get_somatic_mutation(somatic_mut_f)
    somatic_amp, somatic_del = prostate_data_loaders.get_somatic_amp_and_del(somatic_cnv_f)
    germline_mut = prostate_data_loaders.get_germline_mutation(germline_vars_f)  
    y = prostate_data_loaders.get_target(id_map_f, sample_metadata_f, id_to_use="Tumor_Sample_Barcode", target_col="is_met")
    # TODO: deal with adding confounder / "additional" dataset(s): get_confounders(), etc.

    if USE_ONLY_PAIRED or CONVERT_IDS_TO: # Need to test the function going both ways, to somatic and to germline
        prostate_data_loaders.harmonize_prostate_ids(datasets_w_germline_ids=[germline_mut], 
                               datasets_w_somatic_ids=[somatic_mut, somatic_amp, somatic_del, y], 
                               convert_ids_to=CONVERT_IDS_TO) # want to run stuff if change_to_somatic_ids or paired
    if USE_ONLY_PAIRED: # want to run stuff if paired; note that this will only work correctly if the IDs are correctly harmonized
        logging.info("Restrict to overlapping samples (the indices)")
        somatic_mut, somatic_amp, somatic_del, germline_mut, y = data_manipulation.restrict_to_overlapping_indices(somatic_mut, somatic_amp, somatic_del, 
                                                                                                 germline_mut, 
                                                                                                 y)

    # zero impute dataset columns (genes) as necessary (maybe have a reference dataset vs all others? Unsure of best way to parameterize this function)
    germline_mut = prostate_data_loaders.zero_impute_germline_datasets(germline_datasets=[germline_mut], 
                                  somatic_datasets=[somatic_mut, somatic_amp, somatic_del], 
                                  zero_impute_germline=ZERO_IMPUTE_GERMLINE) 
    somatic_mut, somatic_amp, somatic_del = prostate_data_loaders.zero_impute_somatic_datasets(germline_datasets=[germline_mut], 
                                 somatic_datasets=[somatic_mut, somatic_amp, somatic_del], 
                                 zero_impute_somatic=ZERO_IMPUTE_SOMATIC)


    logging.info("Now that we have processed all our datasets, we restrict to just the overlapping genes (the columns) (and further filter to those that fit some TCGA criteria)")
    somatic_mut, somatic_amp, somatic_del, germline_mut = prostate_data_loaders.restrict_to_genes_in_common(somatic_mut, somatic_amp, somatic_del, germline_mut)
    
    
    # TODO: change so that the function call is inside a logging statement.
    df_dict = dict(zip(
        ['somatic_mut', 'somatic_amp', 'somatic_del', 'germline_mut', 'y'], 
        [somatic_mut, somatic_amp, somatic_del, germline_mut, y]
    ))
    report_and_eval.report_df_info_with_names(df_dict, n=5)


    logging.info("Finished preparing data format. Now left with: making train/val/test splits, setting up model, running model, evaluate model")

    ####------ below here, working on slowly getting reworking the code so we fit into the above function calls

    # TODO: temporarily include this information so we can directly compare to previous implementation
    logging.info("restricting metadata to samples with both germline and somatic samples")
    sample_metadata = sample_metadata.dropna(subset=["vcf_germline_id", "Tumor_Sample_Barcode"])
    logging.info(f"We have {len(sample_metadata)} pairs between the germline and somatic data")
    logging.info(f"These are the {len([i for i in somatic_mut.index.tolist() if i not in sample_metadata.Tumor_Sample_Barcode.tolist()])} unpaired samples")
    logging.debug(f"samples: {[i for i in somatic_mut.index.tolist() if i not in sample_metadata.Tumor_Sample_Barcode.tolist()]}")

    
    logging.info("# Generate the PNET loader")
    PNET_SPLITS_DIR = "../../pnet_germline/data/pnet_database/prostate/splits"
    TRAIN_SET_INDS_F = os.path.join(PNET_SPLITS_DIR, "training_set.csv")
    VALIDATION_SET_INDS_F = os.path.join(PNET_SPLITS_DIR, "validation_set.csv")
    DATASETS_TO_USE = ['somatic_amp', 'somatic_del', 'somatic_mut', 'germline_mut']
    genetic_data = {
                    'somatic_amp': somatic_amp, 
                    'somatic_del': somatic_del,
                    'somatic_mut': somatic_mut,
                    'germline_mut': germline_mut
                   }

    genetic_data = {key: genetic_data[key] for key in DATASETS_TO_USE if key in genetic_data}
    logging.info(f"Dictionary keys of datasets we will use: {genetic_data.keys()}")

    training_inds = pd.read_csv(TRAIN_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    validation_inds = pd.read_csv(VALIDATION_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    # test_inds = pd.read_csv(os.path.join(PNET_SPLITS_DIR, "test_set.csv"), usecols=["id","response"], index_col="id").index.tolist()
                        

    logging.info("# Train with run()")
    logging.info("P-NET PAPER SPLIT: what are the results using the same data split as in the P-NET paper, albeit with fewer samples?")
    train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, y,
                                                                 train_inds=training_inds,
                                                                 test_inds=validation_inds)

    # logging.info("POSITIVE CONTROL: can we overfit to the training data if we set many of the feature columns equal to the target column?")
    # train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, y,
    #                                                              add_N_perfectly_correlated_with_target=50,
    #                                                              train_inds=training_inds,
    #                                                              test_inds=validation_inds
    #                                                              )

    logging.info("Build the Reactome network structure")
    reactome_network = ReactomeNetwork.ReactomeNetwork(train_dataset.get_genes())

    logging.info("Define the hyperparameters of your modeling run")
    hparams={
        'reactome_network':reactome_network, 
        'nbr_gene_inputs':len(genetic_data), 
        'dropout':0.2, 
        'additional_dims':0, 
        'output_dim':1,
        'lr':1e-3, 
        'weight_decay':1e-5,
        'epochs':1000,
        'early_stopping':False,
        'batch_size':256,
        'verbose':True,
        'zero_impute_germline':ZERO_IMPUTE_GERMLINE,
        'zero_impute_somatic':ZERO_IMPUTE_SOMATIC,    
        'train_set_indices_f':TRAIN_SET_INDS_F,
        'validation_set_indices_f':VALIDATION_SET_INDS_F,
        'train_set_indices':training_inds,
        'validation_set_indices':validation_inds,
        'restricted_to_pairs':USE_ONLY_PAIRED
    }
    
    run = wandb.init(
        # Set the project where this run will be logged
        project="prostate_met_status",
        name="pnet_germline_somatic_paired",
        # Track hyperparameters and run metadata
        config=hparams
    )

    logging.info("make data loaders")
    train_loader, val_loader = pnet_loader.to_dataloader(train_dataset, test_dataset, hparams['batch_size'])

    logging.info(f"run PNET")
    model = Pnet.PNET_NN(reactome_network=hparams['reactome_network'], 
                         nbr_gene_inputs=hparams['nbr_gene_inputs'], 
                         output_dim=hparams['output_dim'],
                         dropout=hparams['dropout'],
                         additional_dims=hparams['additional_dims'],
                         lr=hparams['lr'], 
                         weight_decay=hparams['weight_decay'])

    
    model, train_losses, test_losses = Pnet.train(model, train_loader, val_loader, epochs=hparams['epochs'], verbose=hparams['verbose'], early_stopping=hparams['early_stopping'])
    logging.info("Check model convergence by examining the plot of how loss changes over time")
    plt = report_and_eval.get_loss_plot(train_losses=train_losses, test_losses=test_losses)
    # plt.show() # TODO: should also save this plot somehow, or send to W&B
    # instead of doing plt.show() do: # see https://docs.wandb.ai/guides/integrations/scikit
    wandb.log({"convergence plot": plt})


    logging.info("Get model predictions")
    y_train_preds, y_train_probas, y_test_preds, y_test_probas = report_and_eval.get_model_preds_and_probs(model, train_dataset, test_dataset)

    logging.info("Get train performance metrics") # TODO    
    train_acc, train_cm = report_and_eval.get_performance_metrics(who="train", y_trues=train_dataset.y,
                                            y_preds=y_train_preds, y_probas=y_train_probas)

    logging.info("Get test performance metrics")
    test_acc, test_cm, = report_and_eval.get_performance_metrics(who="val", y_trues=test_dataset.y, # TODO: hard-coded as validation
                                            y_preds=y_test_preds, y_probas=y_test_probas)

    report_and_eval.get_performance_metrics_wandb(model, # TODO: start here. how do I exactly get x_train? Is it train_dataset.x? What about train_dataset.additional?
                                                  train_dataset.x, train_dataset.y, y_train_preds, 
                                                  test_dataset.x, test_dataset.y, y_test_preds) 

    # TODO: I think all of the following is redundant. However, some of the print statements and plots are useful for debugging
    # train_preds = torch.sigmoid(model(train_dataset.x, train_dataset.additional)).detach().numpy()
    # plt.hist(train_preds)

    # test_preds = torch.sigmoid(model(test_dataset.x, test_dataset.additional)).detach().numpy()
    # plt.hist(test_preds)

    # print("train set metrics")
    # cm_train = confusion_matrix(train_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in train_preds])
    # print(classification_report(train_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in train_preds]))
    # print(roc_auc_score(train_dataset.y.detach().numpy(), train_preds))

    # print(cm_train)

    # print("\nvalidation set metrics")
    # cm_test = confusion_matrix(test_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in test_preds]) # can also do (test_preds >= 0.5) instead of the list comprehension
    # print(classification_report(test_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in test_preds])) 
    # print(roc_auc_score(test_dataset.y.detach().numpy(), test_preds))
    # print(cm_test)
    
    logging.info("ending wandb run")
    wandb.finish()
    return


if __name__=="__main__":
    main()