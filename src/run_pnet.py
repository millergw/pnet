# Script to run Marc Glettig's P-NET on prostate germline and/or somatic data
# Author: Gwen Miller
# Generated by copying over and then editing everything in the file: `pnet/notebooks/prostate_metastatic_prediction_example.ipynb`

import os
import sys

####### TODO: figure out module importing 
sys.path.append("..")

from src import pnet_loader
from src import util
from src import ReactomeNetwork # this loads
from src import Pnet # this fails to load bc of how it imports ReactomeNetwork...

# Add the path "../pnet_germline/src" to the upstream script's directory
upstream_dir = os.path.join('../..', 'pnet_germline', 'src')
sys.path.append(upstream_dir)
import utils # from Gwen's repo


import wandb
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt


# Importing packages related to model performance
from sklearn.metrics import confusion_matrix # expects true_labels, predicted_labels
from sklearn.metrics import classification_report # expects true_labels, predicted_labels
from sklearn.metrics import roc_auc_score # expects true_labels, predicted_probs
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import accuracy_score
import torch.nn as nn

import logging
logging.basicConfig(
            filename='run_pnet.log', 
            encoding='utf-8',
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.INFO,
            datefmt='%Y-%m-%d %H:%M:%S')

logger = logging.getLogger()
logger.setLevel(logging.INFO)


mutations_dict = {"3'Flank": 'Silent',
                  "5'Flank": 'Silent',
                  "5'UTR": 'Silent',
                  "3'UTR": 'Silent',
                  "IGR": 'Silent',
                  "Intron": 'Silent',
                  "lincRNA": 'Silent',
                  "RNA": 'Silent',
                  "Silent": 'Silent',
                  "non_coding_transcript_exon": 'Silent',
                  "upstream_gene": 'Silent',
                  "Splice_Region": 'Silent',
                  "Targeted_Region": 'Silent',
                  'Splice_Site': 'LOF',
                  'Nonsense_Mutation': 'LOF',
                  'Frame_Shift_Del': 'LOF',
                  'Frame_Shift_Ins': 'LOF',
                  'Stop_Codon_Del': 'LOF',
                  'Stop_Codon_Ins': 'LOF',
                  'Nonstop_Mutation': 'LOF',
                  'Start_Codon_Del': 'LOF',
                  'Missense_Mutation': 'Other_nonsynonymous',
                  'In_Frame_Del': 'Other_nonsynonymous',
                  'In_Frame_Ins': 'Other_nonsynonymous',
                  'De_novo_Start_InFrame': 'Other_nonsynonymous',
                  'De_novo_Start_OutOfFrame': 'Other_nonsynonymous',
                  'Start_Codon_Ins': 'Other_nonsynonymous'
                  }


def get_somatic_mutation():
    logging.info("Getting somatic mutation data")
    somatic_mut = load_somatic_mut()
    somatic_mut = format_mutation_data(somatic_mut)
    return somatic_mut


def get_germline_mutation():
    logging.info("Getting germline mutation data")
    germline_var_df = load_germline_mut()
    germline_mut = format_germline_mutation_data(germline_var_df)
    return germline_mut


def get_somatic_amp_and_del():
    logging.info("getting somatic amplification and deletion data")
    cnv = load_somatic_cnv()
    somatic_amp = format_cnv_data(cnv, data_type='cnv_amp', cnv_levels=3, cnv_filter_single_event=True, mut_binary=False, selected_genes=None)
    somatic_del = format_cnv_data(cnv, data_type='cnv_del', cnv_levels=3, cnv_filter_single_event=True, mut_binary=False, selected_genes=None)
    return somatic_amp, somatic_del  


def get_target(id_map_f, sample_metadata_f, id_to_use = "Tumor_Sample_Barcode", target_col="is_met"):
    """
    Get a DF of the target variable indexed by a sample ID.
    """
    logging.info("Getting prediction target") # TODO: alter to work when we aren't using paired samples
    sample_metadata = load_sample_metadata_and_target(id_map_f, sample_metadata_f)
    target = extract_target(sample_metadata, id_to_use=id_to_use, target_col="is_met")
    logging.info(f"Target column value_counts: {target[target_col].value_counts()}")
    return target


def load_sample_metadata_and_target(id_map_f, sample_metadata_f):
    # TODO: start here. Check if running this gets us to have all the columns as we would with the next ~7 lines of code; I think the output will be equivalent after we restrict to paired samples!

    logging.info("Loading the sample metadata DF that has all the IDs and also our target, metastatic status ('is_met')")
    sample_metadata = utils.load_sample_metadata_with_all_germline_ids(sample_metadata_f, id_map_f) # TODO: start here. Can we remove reliance on this function, and use a simpler one instead?
    logging.debug(sample_metadata.head())
    logging.debug(sample_metadata.shape)
    # sample_metadata = utils.load_sample_metadata_with_all_germline_ids(sample_metadata_f, germline_id_map_f)
    # logging.debug(sample_metadata.head())

    # logging.info("Adding the mapping columns that let us pair germline with somatic in the future (namely the vcf_germline_id (germline) and the Tumor_Sample_Barcode (P-NET somatic)")
    # sample_metadata = pd.merge(germline_somatic_id_map, sample_metadata, right_index=True, left_on="sample_metadata_germline_id",
    #                suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)') # TODO: this is slick code to drop duplicate columns after a merge
    # logging.debug(sample_metadata.head())
    # logging.debug(sample_metadata.shape)

    return sample_metadata


def extract_target(df, id_to_use = "Tumor_Sample_Barcode", target_col = "is_met"):
    assert id_to_use in df.columns.tolist(), "The ID you wanted to use isn't in the DF columns" # e.g. "Tumor_Sample_Barcode", "vcf_germline_ids"

    logging.info("Generating the target DF (target column '{target_col}' indexed by '{id}')")
    target = df.set_index(id_to_use).loc[:, [target_col]]
    logging.debug(target.head())
    logging.debug(len(target))
    logging.debug(f"target value_counts: {target[target_col].value_counts()}")
    return target


def load_somatic_mut(somatic_mut_f):
    logging.info(f"Load somatic mutation data from {somatic_mut_f}")
    somatic_mut = load_df_verbose(somatic_mut_f)
    somatic_mut.set_index('Tumor_Sample_Barcode', inplace=True)
    return somatic_mut


def load_somatic_cnv(somatic_cnv_f):
    logging.info(f"Load somatic CNV data from {somatic_cnv_f}")
    cnv = load_df_verbose(somatic_cnv_f)
    cnv.rename(columns={"Unnamed: 0": "Tumor_Sample_Barcode"}, inplace=True)
    cnv.set_index('Tumor_Sample_Barcode', inplace=True)
    return cnv


def load_somatic_response(response_f): # TODO: this function might be made redundant by the get_target function. 
    # TODO: this isn't the optimal way; I think I should start from one of my metadata matrices
    logging.info(f"load somatic response data from {response_f}")
    response = load_df_verbose(response_f)
    response.rename(columns={'id': "Tumor_Sample_Barcode"}, inplace=True)
    response.set_index('Tumor_Sample_Barcode', inplace=True)
    return response


def load_germline_mut(germline_vars_f): # TODO: deal with paths
    logging.info(f"Loading germline mutation data from {germline_vars_f}")
    germline_var_df = pd.read_csv(germline_vars_f, low_memory=False, sep="\t")
    germline_var_df = germline_var_df.set_index("Uploaded_variation")
    logging.info(f"Shape of raw germline mutation data: {germline_var_df.shape()}")
    return germline_var_df


def find_overlapping_columns(*dataframes):
    # Ensure that at least two DataFrames are provided
    if len(dataframes) < 2:
        raise ValueError("At least two DataFrames are required for finding overlaps.")

    # Get the columns of the first DataFrame
    overlapping_columns = set(dataframes[0].columns)

    # Find the intersection of columns with each subsequent DataFrame
    for df in dataframes[1:]:
        overlapping_columns = overlapping_columns.intersection(df.columns)

    logging.info(f"We found {len(overlapping_columns)} overlapping columns")
    return list(overlapping_columns)


def find_overlapping_indices(*dataframes):
    # Ensure that at least two DataFrames are provided
    if len(dataframes) < 2:
        raise ValueError("At least two DataFrames are required for finding overlaps.")

    # Get the indices of the first DataFrame
    overlapping_indices = set(dataframes[0].index)

    # Find the intersection of indices with each subsequent DataFrame
    for df in dataframes[1:]:
        overlapping_indices = overlapping_indices.intersection(df.index)
        
    logging.info(f"We found {len(overlapping_indices)} overlapping indices")
    return list(overlapping_indices)


def find_overlapping_elements(*arrays):
    # Ensure that at least two arrays are provided
    if len(arrays) < 2:
        raise ValueError("At least two arrays are required for finding overlaps.")

    # Get the elements of the first array
    overlapping_elements = set(arrays[0])

    # Find the intersection with each subsequent array
    for a in arrays[1:]:
        overlapping_elements = overlapping_elements.intersection(a)

    logging.info(f"We found {len(overlapping_elements)} overlapping elements")
    return list(overlapping_elements)


def restrict_to_overlapping_indices(*dataframes):
    logging.debug("Find the overlapping indices among all DataFrames")
    overlapping_indices = find_overlapping_indices(*dataframes)
    logging.info(f"The number of overlapping indices amoung the {len(dataframes)} dataframes is {len(overlapping_indices)}.")

    logging.info(f"Restricting each DataFrame to the {len(overlapping_indices)} overlapping indices")
    restricted_dataframes = []
    for df in dataframes:
        logging.debug(f"Shape before: {df.shape}")
        restricted_df = df.loc[overlapping_indices]
        logging.debug(f"Shape after: {restricted_df.shape}")
        restricted_dataframes.append(restricted_df)
    return restricted_dataframes


def restrict_to_overlapping_columns(*dataframes):
    logging.debug("Find the overlapping columns among all DataFrames")
    overlapping_columns = find_overlapping_columns(*dataframes)
    logging.info(f"The number of overlapping columns amoung the {len(dataframes)} dataframes is {len(overlapping_columns)}.")

    logging.debug("Restricting each DataFrame to the overlapping columns")
    restricted_dataframes = []
    for df in dataframes:
        logging.debug(f"Shape before: {df.shape}")
        restricted_df = df[overlapping_columns]
        logging.debug(f"Shape after: {restricted_df.shape}")
        restricted_dataframes.append(restricted_df)
    return restricted_dataframes


def filter_to_specified_indices(indices, *dataframes):
    # Restrict each DataFrame to the specified indices
    restricted_dataframes = []
    for df in dataframes:
        logging.debug(f"Shape before: {df.shape}")
        restricted_df = df.loc[indices]
        logging.debug(f"Shape after: {restricted_df.shape}")
        restricted_dataframes.append(restricted_df)
    return restricted_dataframes


def filter_to_specified_columns(columns, *dataframes):
    # Restrict each DataFrame to the specified columns
    restricted_dataframes = []
    for df in dataframes:
        logging.debug(f"Shape before: {df.shape}")
        restricted_df = df[columns]
        logging.debug(f"Shape after: {restricted_df.shape}")
        restricted_dataframes.append(restricted_df)
    return restricted_dataframes


def get_genes_in_common(*dataframes): # TODO: test function
    logging.info("finding overlapping genes across the DFs")
    overlapping_genes = find_overlapping_columns(dataframes)
    
    logging.info("find the overlap between this and the pre-specified list of TCGA cancer genes and those expressed in the prostate")
    # TODO: looks like there was an excel misshap! MAR1 has become 1-Mar, 10-Sept, etc. But I don't think these genes are covered by our datasets anyway...
    genes = pd.read_csv('../../pnet_germline/data/pnet_database/genes/tcga_prostate_expressed_genes_and_cancer_genes.csv') 
    logging.debug(genes.head())
    overlapping_genes = find_overlapping_elements(set(genes['genes']), overlapping_genes)

    return overlapping_genes


def restrict_to_genes_in_common(*datasets): # TODO: this is equivalent to filtering columns of a DF?? TODO: no longer true, filtering columns. Check the functionality of this.
    """
    Filter the columns of the dataframe(s).
    Args:
    - *datasets (Pandas DF): arbitrary number of DFs with format samples x genes/features
    """
    # return df[genes_in_common].copy()
    genes_in_common = get_genes_in_common(*datasets)
    restricted_dataframes = filter_to_specified_columns(genes_in_common, *datasets)
    return restricted_dataframes


def load_df_verbose(f):
    logging.info(f"loading file at {f}")
    df = pd.read_csv(f)
    logging.debug(df.head())
    logging.debug(df.shape)
    return df


def is_binarized(df):
    """
    Ex:
    data = {'A': [0, 1, 1, 0],
    'B': [0, 0, 1, 1],
    'C': [1, 1, 0, 0]}

    binarized_df = pd.DataFrame(data)
    is_binarized(binarized_df)
    """
    return np.all((df.values == 0.) | (df.values == 1.))


def format_mutation_data(mut_df, mut_binary=True):
    """
    Args:
    - mut_binary: True means that we should binarize the DF if it isn't already binarizied; we want a binary mutation DF.
    If binarized already, then should only have <=2 unique values in each column (0/1). If we have more, then we're probably working with a burden matrix, but this should be investigated."""
    if mut_binary and not is_binarized(mut_df):
    # if mut_binary and max(somatic_mut.nunique()) >= 2:
        logging.info(f"Matrix was not binary. There were {max(mut_df.nunique())} unique values; binarizing now")
        mut_df[mut_df > 1.] = 1.
    return mut_df


def format_germline_mutation_data(df):
    """
    Args:
    - df: variant x [variant metadata and sample-level calls]  (quasi VCF file format)
    """
    logging.info("Starting process of formatting the germline mutation data.")
    logging.info("Extracting the variant metadata DF")
    variant_metadata = utils.get_variant_metadata_from_VCF(df)

    logging.info("Make the binary variant-level genotypes matrix (variants x samples)")
    binary_genotypes = utils.make_binary_genotype_mat_from_VCF(df)

    logging.info("Make the binary gene-level genotypes matrix (genes x samples)")
    gene_level_genotype_matrix =  utils.convert_binary_var_mat_to_gene_level_mat(binary_genotypes, 
                                                                variant_metadata, 
                                                                binary_output = True)

    logging.info("Transposing to get samples x genes")
    germline_mut = gene_level_genotype_matrix.T
    return germline_mut


def format_germline_mutation_data_old(df, change_to_somatic_ids=True, paired=True):
    """
    Args:
    - df: samples x variant and features (quasi VCF file format)
    - change_to_somatic_ids: change from the germline IDs to the somatic IDs (NOTE: might get NAs!)
    - paired: if True, we only keep germline samples were we also have the paired somatic sample
    """
    logging.info("Extracting the variant metadata DF")
    variant_metadata = utils.get_variant_metadata_from_VCF(df)

    logging.info("Make the binary variant-level genotypes matrix")
    binary_genotypes = utils.make_binary_genotype_mat_from_VCF(df)

    logging.info("Make the binary gene-level genotypes matrix")
    gene_level_genotype_matrix =  utils.convert_binary_var_mat_to_gene_level_mat(binary_genotypes, 
                                                                variant_metadata, 
                                                                binary_output = True)

    if change_to_somatic_ids or paired:
        logging.info("Changing the sample IDs to match those used with the somatic data")
        # create a dict for mapping 
        germID_to_somaticID = dict(zip(germline_somatic_id_map.vcf_germline_id, germline_somatic_id_map.Tumor_Sample_Barcode))
        gene_level_genotype_matrix.columns = pd.Series(gene_level_genotype_matrix.columns.tolist()).map(germID_to_somaticID).tolist()

    logging.debug("transposing to get genes x samples")
    germline_mut = gene_level_genotype_matrix.T

    if paired:
        logging.info("Removing any rows whose index is NA: we don't currently have the matched germline for this sample")
        logging.info(f"shape before: {germline_mut.shape}")
        germline_mut = germline_mut.reset_index().dropna(subset=["index"]).set_index("index")
        logging.info(f"shape after: {germline_mut.shape}")

    return germline_mut


def harmonize_prostate_ids(datasets_w_germline_ids, datasets_w_somatic_ids, convert_ids_to = "somatic"): # TODO: how should I handle IDs that can't be converted? Right now, I just replace with NAs and warn. But I think I should drop NA rows.
    # TODO: edit third parameter so that it takes either "somatic", "germline", or None
    """
    Args:
    - germline_datasets: any DFs that use the vcf_germline_ids ID as their index
    - somatic_datasets: any DFs that use the Tumor_Sample_Barcode ID as their index
    - convert_ids_to: if "somatic", then convert the germline_datasets IDs to match the somatic_datasets and analogously for "germline". If None, then don't alter the inputs.

    Returns two lists of DataFrames: 
        one with modified DataFrames (either germline or somatic, depending on the value of `convert_ids_to`), 
        and one with the original DataFrames (either somatic or germline, respectively).
    """
    if convert_ids_to  not in ["somatic", "germline", None]:
        raise ValueError(f"The convert_ids_to parameter must be one of 'somatic', 'germline', or None but was input as {str(convert_ids_to)}.")

    if convert_ids_to == "somatic":
        logging.info("Converting germline IDs (vcf_germline_id) to somatic IDs (Tumor_Sample_Barcode)")
        altered_germline_datasets = []
        for df in datasets_w_germline_ids:
            df.index = convert_germline_id_to_somatic_id(df.index.tolist())   
            df = drop_na_index_rows(df)
            altered_germline_datasets.append(df)     
        return altered_germline_datasets, datasets_w_somatic_ids

    elif convert_ids_to == "germline": 
        logging.info("Converting somatic IDs (Tumor_Sample_Barcode) to germline IDs (vcf_germline_id)")
        altered_somatic_datasets = []
        for df in datasets_w_somatic_ids:
            df.index = convert_somatic_id_to_germline_id(df.index.tolist())
            df = drop_na_index_rows(df)
            altered_somatic_datasets.append(df)
        return datasets_w_germline_ids, altered_somatic_datasets
    
    elif convert_ids_to is None:
        logging.info("Returning without converting any sample IDs")
        return datasets_w_germline_ids, datasets_w_somatic_ids


def drop_na_index_rows(df):
    """
    Drop rows with missing (NaN) index values from a pandas DataFrame. This includes np.nan and None.
    
    Parameters:
    df (pd.DataFrame): The DataFrame from which to drop rows with missing index values.
    
    Returns:
    pd.DataFrame: A new DataFrame with rows containing missing index values removed.

    # Example usage:
    data = {'A': [1, 2, 3, 4, 5]}
    index_values = [np.nan, 'row2', 'row3', None, 'row5']
    df = pd.DataFrame(data, index=index_values)

    # Drop rows with missing index values from the DataFrame
    cleaned_df = drop_na_index_rows(df)

    print("Original DataFrame:")
    print(df)

    print("\nDataFrame after dropping rows with missing index values:")
    print(cleaned_df)
    """
    logging.info("Use boolean indexing to drop rows with NaN index values")
    logging.debug(f"Shape before: {df.shape}")
    cleaned_df = df[~df.index.isna()]
    logging.debug(f"Shape after: {cleaned_df.shape}")
    return cleaned_df


def convert_germline_id_to_somatic_id(germlineIDs, GERMLINE_DATADIR = "../../pnet_germline/data/"):
    """
    Convert list of germline IDs (vcf_germline_id) to somatic IDs (Tumor_Sample_Barcode).
    Warn if the converted list has any NAs: this means that a match wasn't found.
    """
    logging.debug("Loading the germline ID and somatic-germline ID mapping DF")
    germline_somatic_id_map_f = os.path.join(GERMLINE_DATADIR, "prostate/germline_somatic_id_map_outer_join.csv")
    germline_somatic_id_map = load_df_verbose(germline_somatic_id_map_f)
    logging.info("Converting list of germline IDs (vcf_germline_id) to somatic IDs (Tumor_Sample_Barcode).")
    somaticIDs = convert_values(input_value=germlineIDs, 
                                source=germline_somatic_id_map.vcf_germline_id.tolist(), 
                                target=germline_somatic_id_map.Tumor_Sample_Barcode.tolist())
    return somaticIDs


def convert_somatic_id_to_germline_id(somaticIDs, GERMLINE_DATADIR = "../../pnet_germline/data/"):
    """
    Convert list of somatic IDs (Tumor_Sample_Barcode) to germline IDs (vcf_germline_id).
    Warn if the converted list has any NAs: this means that a match wasn't found.
    """
    logging.debug("Loading the germline ID and somatic-germline ID mapping DF")
    germline_somatic_id_map_f = os.path.join(GERMLINE_DATADIR, "prostate/germline_somatic_id_map_outer_join.csv")
    germline_somatic_id_map = load_df_verbose(germline_somatic_id_map_f)
    logging.info("Converting list of somatic IDs (Tumor_Sample_Barcode) to germline IDs (vcf_germline_id).")
    germlineIDs = convert_values(input_value=somaticIDs, 
                                source=germline_somatic_id_map.Tumor_Sample_Barcode.tolist(),
                                target=germline_somatic_id_map.vcf_germline_id.tolist() 
                                )
    return germlineIDs


def convert_values(input_value, source, target):
    """
    # Example usage:
    value1_list = ['apple', 'banana', 'cherry']
    value2_list = ['red', 'yellow', 'red']

    # Convert a single value
    conversion_result = convert_values('kiwi', value1_list, value2_list)
    print(f"Converted List: {conversion_result}")
    # > Converted List: 'yellow'
    # Convert a list of values
    input_list = ['apple', 'banana', 'kiwi']
    conversion_result = convert_values(input_list, value1_list, value2_list)
    print(f"Converted List: {conversion_result}")
    # > Converted List: ['red', 'yellow', 'kiwi']
    """

    logging.debug("Ensure source and target have the same length")
    if len(source) != len(target):
        raise ValueError("Input lists must have the same length.")

    logging.info("Converting input by creating a dictionary to map values from 'source' to 'target'")
    value_mapping = dict(zip(source, target))

    logging.debug("Initialize lists to track converted and unconverted items")
    converted_items = []
    unconverted_items = []

    if isinstance(input_value, list):
        logging.debug("If input_value is a list, convert each element")
        for item in input_value:
            converted_value = value_mapping.get(item, None)
            if converted_value is not None:
                converted_items.append(converted_value)
            else:
                converted_items.append(np.nan)
                unconverted_items.append(item)
    else:
        logging.debug("If input_value is a single value, convert it")
        converted_value = value_mapping.get(input_value, None)
        if converted_value is not None:
            converted_items.append(converted_value)
        else:
            converted_items.append(np.nan)
            unconverted_items.append(input_value)

    logging.debug("{len(converted_items)} converted: {converted_items}")
    if len(unconverted_items)>0:
        logging.warn("{len(unconverted_items)} couldn't be converted: {unconverted_items}")
    return converted_items


def convert_values_old(input_value, source, target):
    """
    # Example usage
    source = ['apple', 'banana', 'cherry']
    target = ['red', 'yellow', 'red']

    # Convert a single value
    converted_value = convert_values('banana', source, target)
    print(f"Converted Value: {converted_value}") 
    # > Converted Value: 'yellow'

    # Convert a list of values
    input_list = ['apple', 'banana', 'kiwi']
    converted_list = convert_values(input_list, source, target)
    print(f"Converted List: {converted_list}")
    # > Converted List: ['red', 'yellow', 'kiwi']
    """
    logging.info("Converting input by creating a dictionary to map values from 'source' to 'target'")
    value_mapping = dict(zip(source, target))

    if isinstance(input_value, list):
        logging.debug("If input_value is a list, convert each element")
        output = list(map(lambda x: value_mapping.get(x, np.nan), input_value))
    else:
        logging.debug("If input_value is a single value, convert it")
        output =  value_mapping.get(input_value, np.nan)
    
    if np.nan in output:
        logging.warn("The converted list contains np.nan values.")
    return output


def impute_cols_with_a_constant(df, new_col_names, fill=0):
    new_col_names = list(new_col_names)
    logging.info(f"Shape before {fill}-imputation: {df.shape}")
    logging.info(f"We have {len(new_col_names)} features to add as a column of all {fill}'s")
    df = df.reindex(columns=df.columns.tolist() + new_col_names).fillna(fill)
    logging.info(f"Shape after {fill}-imputation: {df.shape}")
    return df


def impute_cols_with_a_constant_v2(new_col_names, fill=0, *dataframes):
    """The point here is to be able to pass mutiple dataframes in"""
    new_col_names = list(new_col_names)
    imputed_dataframes = []
    for df in dataframes:
        logging.info(f"Shape before {fill}-imputation: {df.shape}")
        logging.info(f"We have {len(new_col_names)} features to add as a column of all {fill}'s")
        imputed_df = df.reindex(columns=df.columns.tolist() + new_col_names).fillna(fill)
        logging.info(f"Shape after {fill}-imputation: {imputed_df.shape}")
        imputed_dataframes.append(imputed_df)
    return dataframes


def zero_impute_somatic_datasets(germline_datasets, somatic_datasets, zero_impute_somatic = False): # TODO: test
    if zero_impute_somatic is True:
        logging.info(f"Starting process of zero-imputing the {len(somatic_datasets)} somatic dataset(s)")
        germline_features = set(find_overlapping_columns(*germline_datasets))
        imputed_dataframes = []
        for df in somatic_datasets:
            features_only_in_germline = germline_features - set(df.columns)
            imputed_df = impute_cols_with_a_constant(df, features_only_in_germline, fill=0)
            imputed_dataframes.append(imputed_df)
        return imputed_dataframes
    return somatic_datasets
    

def zero_impute_germline_datasets(germline_datasets, somatic_datasets, zero_impute_germline = True): # TODO: this is basically the same as the somatic version of the function; could make a more general single function
    if zero_impute_germline is True:
        """
        Return: returns zero-imputed germline datasets if the zero_imput_germline flag is True. Otherwise, returns the unaltered datasets.

        Example of germline zero-imputation: 
        For genes that are in the somatic dataset but not germline, add colum of zeros to the germline dataset. 
        This way, we will keep all of the somatic data when we run P-NET 
        (not subset down to whatever germline gene subset we're using).
        """
        logging.info(f"Starting process of zero-imputing the {len(germline_datasets)} germline datasets")
        somatic_features = set(find_overlapping_columns(*somatic_datasets))
        imputed_dataframes = []
        for df in germline_datasets:
            features_only_in_somatic = somatic_features - set(df.columns)
            imputed_df = impute_cols_with_a_constant(df, features_only_in_somatic, fill=0)
            imputed_dataframes.append(imputed_df)
        return imputed_dataframes
    return germline_datasets
    

# pulled and modified from the P-NET paper code, data_reader.py script > load_data_type function.
def format_cnv_data(x, data_type='cnv', cnv_levels=5, cnv_filter_single_event=True, mut_binary=False, selected_genes=None):
    logging.info('formatting {}'.format(data_type))
    x = x.copy()
    
    if data_type == 'cnv':
        if cnv_levels == 3:
            logging.info('cnv_levels = 3')
            # remove single amplification and single delteion, they are usually noisey
            if cnv_levels == 3:
                if cnv_filter_single_event:
                    x[x == -1.] = 0.0
                    x[x == -2.] = 1.0
                    x[x == 1.] = 0.0
                    x[x == 2.] = 1.0
                else:
                    x[x < 0.] = -1.
                    x[x > 0.] = 1.

    if data_type == 'cnv_del':
        x[x >= 0.0] = 0.
        if cnv_levels == 3:
            if cnv_filter_single_event:
                x[x == -1.] = 0.0
                x[x == -2.] = 1.0
            else:
                x[x < 0.0] = 1.0
        else:  # cnv == 5 , use everything
            x[x == -1.] = 0.5
            x[x == -2.] = 1.0

    if data_type == 'cnv_amp':
        x[x <= 0.0] = 0.
        if cnv_levels == 3:
            if cnv_filter_single_event:
                x[x == 1.0] = 0.0
                x[x == 2.0] = 1.0
            else:
                x[x > 0.0] = 1.0
        else:  # cnv == 5 , use everything
            x[x == 1.] = 0.5
            x[x == 2.] = 1.0

    if data_type == 'cnv_single_del':
        x, response, info, genes = load_data(cnv_filename, selected_genes)
        x[x == -1.] = 1.0
        x[x != -1.] = 0.0
    if data_type == 'cnv_single_amp':
        x, response, info, genes = load_data(cnv_filename, selected_genes)
        x[x == 1.] = 1.0
        x[x != 1.] = 0.0
    if data_type == 'cnv_high_amp':
        x, response, info, genes = load_data(cnv_filename, selected_genes)
        x[x == 2.] = 1.0
        x[x != 2.] = 0.0
    if data_type == 'cnv_deep_del':
        x, response, info, genes = load_data(cnv_filename, selected_genes)
        x[x == -2.] = 1.0
        x[x != -2.] = 0.0
    return x


def get_pnet_preds_and_probs(pnet_dataset, model):
    probs = torch.sigmoid(model(pnet_dataset.x, pnet_dataset.additional)).detach().numpy()
    preds = [1 if i>0.5 else 0 for i in probs] # TODO: is this the best approach?
    return preds, probs


def get_loss_plot(train_losses, test_losses, 
                  train_label="Train loss", test_label="Validation loss"):
    # Sample data
    epochs = range(1, len(train_losses)+1)

    # Plotting the lines
    plt.plot(epochs, train_losses, label=train_label)
    plt.plot(epochs, test_losses, label=test_label)

    # Adding labels and title
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Model Loss')

    # Adding a legend
    plt.legend()
    return plt
    

def get_performance_metrics(model, train_dataset, test_dataset, config):
    
    x_train = train_dataset.x
    additional_train = train_dataset.additional
    y_train = train_dataset.y
    x_test = test_dataset.x
    additional_test = test_dataset.additional
    y_test = test_dataset.y
    

    logging.info("# making predictions on train and test sets")  
    logging.info("computing model predictions on train set")
    y_train_preds, y_train_probas = get_pnet_preds_and_probs(train_dataset, model)
    plt.hist(y_train_preds)

    logging.info("computing model predictions on test set")
    y_test_preds, y_test_probas = get_pnet_preds_and_probs(test_dataset, model)
    plt.hist(y_test_preds)


    logging.info("# calculating and logging useful performance metrics")
    train_score = accuracy_score(y_train, y_train_preds, normalize=True)
    test_score = accuracy_score(y_test, y_test_preds, normalize=True)
    wandb.run.summary["train_score"] = train_score # we use wandb.run.summary instead of wandb.log when we don't want multiple time steps
    wandb.run.summary["test_score"] = test_score
    wandb.run.summary["test_roc_auc_score"] = roc_auc_score(y_test, y_test_probas) # expects true_labels, predicted_probs
    wandb.run.summary['train_balanced_acc'] = balanced_accuracy_score(y_train, y_train_preds)
    wandb.run.summary['test_balanced_acc'] = balanced_accuracy_score(y_test, y_test_preds)
    
    logging.info(f"train score: {train_score}, test_score: {test_score}")
    logging.info(confusion_matrix(y_test, y_test_preds))
    
    wandb.log({
        "train_confusion_matrix": plot_confusion_matrix(y_train, y_train_preds), # expects true_labels, predicted_labels
        "test_confusion_matrix": confusion_matrix(y_test, y_test_preds),
        "test_classification_report": classification_report(y_test, y_test_preds) # expects true_labels, predicted_labels
              })
    
    wandb.sklearn.plot_confusion_matrix(y_test, y_test_preds)
    wandb.sklearn.plot_summary_metrics(model, x_train, y_train, x_test, y_test)
    
    # TODO: delete the stuff below here; I think it is redundant
    logging.info("train set metrics")
    cm_train = confusion_matrix(y_train, y_train_preds)
    print(classification_report(y_train, y_train_preds))
    print(roc_auc_score(y_train, y_train_probas))
    print(cm_train)

    logging.info("validation set metrics")
    cm_test = confusion_matrix(y_test, y_test_preds) # can also do (test_preds >= 0.5) instead of the list comprehension
    print(classification_report(y_test, y_test_preds)) 
    print(roc_auc_score(y_test, y_test_probas))
    print(cm_test)

    logging.info("# returning the predictions and prediction probabilities for train/test sets")
    return y_train_preds, y_test_preds, y_train_probas, y_test_probas


def report_df_info(*dataframes, n=5): # TODO: use this instead of load_df_verbose
    """
    Report information about an arbitrary number of dataframes.

    Parameters:
    *dataframes (pd.DataFrame): Arbitrary number of dataframes to report information about.
    n (int): Number of columns and indices to display.

    Returns:
    None

    # Example usage:
    data1 = {'A': [1, 2, 3], 'B': [4, 5, 6]}
    data2 = {'X': [7, 8, 9], 'Y': [10, 11, 12]}

    df1 = pd.DataFrame(data1, index=['row1', 'row2', 'row3'])
    df2 = pd.DataFrame(data2, index=['row4', 'row5', 'row6'])

    # Call the function to report information about the dataframes
    report_df_info(df1, df2)
    """

    for idx, df in enumerate(dataframes, start=1):
        print(f"----- DataFrame {idx} Info -----")
        print(f"Shape: {df.shape}")
        print(f"First {n} columns: {df.columns[:n].tolist()}")
        print(f"First {n} indices: {df.index[:n].tolist()}")
        print("-----")
    return


def report_df_info_with_names(df_dict, n=5):
    """
    Report information about dataframes (with DF names provided in a dictionary for convenience).

    Parameters:
    df_dict (dict): A dictionary where keys are names and values are dataframes.
    n (int): Number of columns and indices to display.

    Returns:
    None

    # Example usage:
    data1 = {'A': [1, 2, 3], 'B': [4, 5, 6]}
    data2 = {'X': [7, 8, 9], 'Y': [10, 11, 12]}

    df1 = pd.DataFrame(data1, index=['row1', 'row2', 'row3'])
    df2 = pd.DataFrame(data2, index=['row4', 'row5', 'row6'])

    # Create a dictionary with dataframe names
    dataframes_dict = {"DataFrame 1": df1, "DataFrame 2": df2}

    # Call the function with the dictionary of dataframes
    report_df_info_with_names(dataframes_dict)

    # Alternatively, use dict(zip()) instead of writing out a dictionary
    names = ['Dataframe 1', 'DF2']
    dfs = [df1, df2]
    report_df_info_with_names(dict(zip(names, dfs)))
    """

    for name, df in df_dict.items():
        print(f"----- DataFrame {name} Info -----")
        print(f"Shape: {df.shape}")
        print(f"First {n} columns: {df.columns[:n].tolist()}")
        print(f"First {n} indices: {df.index[:n].tolist()}")
        print("-----")
    return


def main():
    wandb.login()
    """# Load each of your data modalities of interest.
    Format should be samples x genes. Set the sample IDs as the index.

    Data modalities:
    1. somatic amp
    1. somatic del
    1. somatic mut
    1. germline mut (subset to a small number of genes).

    Our somatic data has information for many more genes compared to the germline data. We will need to either:
    1. impute zeros for the excluded germline genes, or 
    2. subset the somatic datasets down to the ones that overlap with the germline data.


    We will be subsetting to the ~1060 samples that we have matched somatic and germline data for.

    NOTE: the PNET loader will automatically restrict the input data modalities to overlapping samples and overlapping genes. 
    This means we will need to be carefull with our input germline dataset if we want to keep all the somatic data."""
    
    # TODO: eventually, want this overall structure of hyperparameters and function calls
    USE_ONLY_PAIRED = True
    CONVERT_IDS_TO = "somatic"
    ZERO_IMPUTE_GERMLINE = True
    ZERO_IMPUTE_SOMATIC = False

    logging.debug("Defining paths for somatic data")
    SOMATIC_DATADIR = "../../pnet_germline/data/pnet_database/prostate/processed"
    somatic_mut_f = os.path.join(SOMATIC_DATADIR, "P1000_final_analysis_set_cross_important_only.csv")
    somatic_cnv_f = os.path.join(SOMATIC_DATADIR, "P1000_data_CNA_paper.csv") # TODO: check how PNET implements splitting this into amp vs del
    
    logging.debug("Defining paths for germline data")
    GERMLINE_DATADIR = "../../pnet_germline/data/"
    # germline_vars_f = os.path.join(GERMLINE_DATADIR, "prostate/prostate_germline_vcf_subset_to_germline_tier_1and2_pathogenic_vars_only.txt")
    germline_vars_f = os.path.join(GERMLINE_DATADIR, "prostate/prostate_germline_vcf_subset_to_germline_tier_12_and_somatic_pathogenic_vars_only.txt")

    logging.debug("Defining paths for the sample metadata")
    id_map_f = os.path.join(GERMLINE_DATADIR, "prostate/germline_somatic_id_map_outer_join.csv") # germline_somatic_id_map_f
    sample_metadata_f = os.path.join(GERMLINE_DATADIR,"prostate/pathogenic_variants_with_clinical_annotation_1341_aug2021_correlation.csv")

    somatic_mut = get_somatic_mutation(somatic_mut_f)
    somatic_amp, somatic_del = get_somatic_amp_and_del(somatic_cnv_f)
    germline_mut = get_germline_mutation(germline_vars_f)  
    y = get_target(id_map_f, sample_metadata_f, id_to_use="Tumor_Sample_Barcode", target_col="is_met")
    # TODO: deal with adding confounder dataset: get_confounders(), etc.

    if USE_ONLY_PAIRED or CONVERT_IDS_TO: # Need to test the function going both ways, to somatic and to germline
        harmonize_prostate_ids(datasets_w_germline_ids=[germline_mut], 
                               datasets_w_somatic_ids=[somatic_mut, somatic_amp, somatic_del, y], 
                               convert_ids_to=CONVERT_IDS_TO) # want to run stuff if change_to_somatic_ids or paired
    if USE_ONLY_PAIRED: # want to run stuff if paired; note that this will only work correctly if the IDs are correctly harmonized
        logging.info("Restrict to overlapping samples (the indices)")
        somatic_mut, somatic_amp, somatic_del, germline_mut, y = restrict_to_overlapping_indices(somatic_mut, somatic_amp, somatic_del, 
                                                                                                 germline_mut, 
                                                                                                 y)

    # zero impute dataset columns (genes) as necessary (maybe have a reference dataset vs all others? Unsure of best way to parameterize this function)
    germline_mut = zero_impute_germline_datasets(germline_datasets=[germline_mut], 
                                  somatic_datasets=[somatic_mut, somatic_amp, somatic_del], 
                                  zero_impute_germline=ZERO_IMPUTE_GERMLINE) 
    somatic_mut, somatic_amp, somatic_del = zero_impute_somatic_datasets(germline_datasets=[germline_mut], 
                                 somatic_datasets=[somatic_mut, somatic_amp, somatic_del], 
                                 zero_impute_somatic=ZERO_IMPUTE_SOMATIC)


    logging.info("Now that we have processed all our datasets, we restrict to just the overlapping genes (the columns) (and further filter to those that fit some TCGA criteria)")
    somatic_mut, somatic_amp, somatic_del, germline_mut = restrict_to_genes_in_common(somatic_mut, somatic_amp, somatic_del, germline_mut)
    
    
    # TODO: change so that the function call is inside a logging statement.
    df_dict = dict(zip(
        ['somatic_mut', 'somatic_amp', 'somatic_del', 'germline_mut', 'y'], 
        [somatic_mut, somatic_amp, somatic_del, germline_mut, y]
    ))
    report_df_info_with_names(df_dict, n=5)


    logging.info("Finished preparing data format. Now left with: making train/val/test splits, setting up model, running model, evaluate model")

    ####------ below here, working on slowly getting reworking the code so we fit into the above function calls

    # TODO: temporarily include this information so we can directly compare to previous implementation
    logging.info("restricting metadata to samples with both germline and somatic samples")
    sample_metadata = sample_metadata.dropna(subset=["vcf_germline_id", "Tumor_Sample_Barcode"])
    logging.info(f"We have {len(sample_metadata)} pairs between the germline and somatic data")
    logging.info(f"These are the {len([i for i in somatic_mut.index.tolist() if i not in sample_metadata.Tumor_Sample_Barcode.tolist()])} unpaired samples")
    logging.debug(f"samples: {[i for i in somatic_mut.index.tolist() if i not in sample_metadata.Tumor_Sample_Barcode.tolist()]}")

    
    logging.info("# Generate the PNET loader")
    PNET_SPLITS_DIR = "../../pnet_germline/data/pnet_database/prostate/splits"
    TRAIN_SET_INDS_F = os.path.join(PNET_SPLITS_DIR, "training_set.csv")
    VALIDATION_SET_INDS_F = os.path.join(PNET_SPLITS_DIR, "validation_set.csv")
    DATASETS_TO_USE = ['somatic_amp', 'somatic_del', 'somatic_mut', 'germline_mut']
    genetic_data = {
                    'somatic_amp': somatic_amp, 
                    'somatic_del': somatic_del,
                    'somatic_mut': somatic_mut,
                    'germline_mut': germline_mut
                   }

    genetic_data = {key: genetic_data[key] for key in DATASETS_TO_USE if key in genetic_data}
    logging.info(f"Dictionary keys of datasets we will use: {genetic_data.keys()}")

    training_inds = pd.read_csv(TRAIN_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    validation_inds = pd.read_csv(VALIDATION_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    # test_inds = pd.read_csv(os.path.join(PNET_SPLITS_DIR, "test_set.csv"), usecols=["id","response"], index_col="id").index.tolist()
                        

    logging.info("# Train with run()")
    logging.info("P-NET PAPER SPLIT: what are the results using the same data split as in the P-NET paper, albeit with fewer samples?")
    train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, y,
                                                                 train_inds=training_inds,
                                                                 test_inds=validation_inds)

    # logging.info("POSITIVE CONTROL: can we overfit to the training data if we set many of the feature columns equal to the target column?")
    # train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, y,
    #                                                              add_N_perfectly_correlated_with_target=50,
    #                                                              train_inds=training_inds,
    #                                                              test_inds=validation_inds
    #                                                              )

    logging.info("Build the Reactome network structure")
    reactome_network = ReactomeNetwork.ReactomeNetwork(train_dataset.get_genes())

    logging.info("Define the hyperparameters of your modeling run")
    hparams={
        'reactome_network':reactome_network, 
        'nbr_gene_inputs':len(genetic_data), 
        'dropout':0.2, 
        'additional_dims':0, 
        'output_dim':1,
        'lr':1e-3, 
        'weight_decay':1e-5,
        'epochs':1000,
        'early_stopping':False,
        'batch_size':256,
        'verbose':True,
        'zero_impute_germline':ZERO_IMPUTE_GERMLINE,
        'zero_impute_somatic':ZERO_IMPUTE_SOMATIC,    
        'train_set_indices_f':TRAIN_SET_INDS_F,
        'validation_set_indices_f':VALIDATION_SET_INDS_F,
        'train_set_indices':training_inds,
        'validation_set_indices':validation_inds,
        'restricted_to_pairs':USE_ONLY_PAIRED
    }
    
    run = wandb.init(
        # Set the project where this run will be logged
        project="prostate_met_status",
        name="pnet_germline_somatic_paired",
        # Track hyperparameters and run metadata
        config=hparams
    )

    logging.info("make data loaders")
    train_loader, val_loader = pnet_loader.to_dataloader(train_dataset, test_dataset, hparams['batch_size'])

    logging.info(f"run PNET")
    model = Pnet.PNET_NN(reactome_network=hparams['reactome_network'], 
                         nbr_gene_inputs=hparams['nbr_gene_inputs'], 
                         output_dim=hparams['output_dim'],
                         dropout=hparams['dropout'],
                         additional_dims=hparams['additional_dims'],
                         lr=hparams['lr'], 
                         weight_decay=hparams['weight_decay'])

    
    logging.info("checking model convergence by examining a plot of how the loss changes over time")
    #for testing mutation not loss [Gwen: what does this mean?]
    model, train_losses, test_losses = Pnet.train(model, train_loader, val_loader, epochs=hparams['epochs'], verbose=hparams['verbose'], early_stopping=hparams['early_stopping'])
    plt = get_loss_plot(train_losses=train_losses, test_losses=test_losses)
    plt.show()

    logging.info("checking model performance")
    logging.info("Checking performance metrics for train/test")
    y_train_preds, y_test_preds, y_train_probas, y_test_probas = get_performance_metrics(model, train_dataset, test_dataset, config=hparams)

    train_preds = torch.sigmoid(model(train_dataset.x, train_dataset.additional)).detach().numpy()
    plt.hist(train_preds)

    test_preds = torch.sigmoid(model(test_dataset.x, test_dataset.additional)).detach().numpy()
    plt.hist(test_preds)

    print("train set metrics")
    cm_train = confusion_matrix(train_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in train_preds])
    print(classification_report(train_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in train_preds]))
    print(roc_auc_score(train_dataset.y.detach().numpy(), train_preds))

    print(cm_train)

    print("\nvalidation set metrics")
    cm_test = confusion_matrix(test_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in test_preds]) # can also do (test_preds >= 0.5) instead of the list comprehension
    print(classification_report(test_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in test_preds])) 
    print(roc_auc_score(test_dataset.y.detach().numpy(), test_preds))
    print(cm_test)
    
    logging.info("ending wandb run")
    wandb.finish()
    return

    
if __name__=="__main__":
    main()