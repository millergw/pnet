# Script to run Marc Glettig's P-NET on prostate germline and/or somatic data
# Author: Gwen Miller
# Generated by copying over and then editing everything in the file: `pnet/notebooks/prostate_metastatic_prediction_example.ipynb`

import os
import sys

####### TODO: figure out module importing 
import pnet_loader
import util
import ReactomeNetwork # this loads
import Pnet # this fails to load bc of how it imports ReactomeNetwork...

# Gwen's scripts
import data_manipulation
import vcf_manipulation
import prostate_data_loaders

import wandb
import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt


# Importing packages related to model performance
from sklearn.metrics import confusion_matrix # expects true_labels, predicted_labels
from sklearn.metrics import classification_report # expects true_labels, predicted_labels
from sklearn.metrics import roc_auc_score # expects true_labels, predicted_probs
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import accuracy_score
import torch.nn as nn

import logging
logging.basicConfig(
            filename='run_pnet.log', 
            encoding='utf-8',
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.INFO,
            datefmt='%Y-%m-%d %H:%M:%S')

logger = logging.getLogger()
logger.setLevel(logging.INFO)


mutations_dict = {"3'Flank": 'Silent',
                  "5'Flank": 'Silent',
                  "5'UTR": 'Silent',
                  "3'UTR": 'Silent',
                  "IGR": 'Silent',
                  "Intron": 'Silent',
                  "lincRNA": 'Silent',
                  "RNA": 'Silent',
                  "Silent": 'Silent',
                  "non_coding_transcript_exon": 'Silent',
                  "upstream_gene": 'Silent',
                  "Splice_Region": 'Silent',
                  "Targeted_Region": 'Silent',
                  'Splice_Site': 'LOF',
                  'Nonsense_Mutation': 'LOF',
                  'Frame_Shift_Del': 'LOF',
                  'Frame_Shift_Ins': 'LOF',
                  'Stop_Codon_Del': 'LOF',
                  'Stop_Codon_Ins': 'LOF',
                  'Nonstop_Mutation': 'LOF',
                  'Start_Codon_Del': 'LOF',
                  'Missense_Mutation': 'Other_nonsynonymous',
                  'In_Frame_Del': 'Other_nonsynonymous',
                  'In_Frame_Ins': 'Other_nonsynonymous',
                  'De_novo_Start_InFrame': 'Other_nonsynonymous',
                  'De_novo_Start_OutOfFrame': 'Other_nonsynonymous',
                  'Start_Codon_Ins': 'Other_nonsynonymous'
                  }


def get_pnet_preds_and_probs(pnet_dataset, model):
    probs = torch.sigmoid(model(pnet_dataset.x, pnet_dataset.additional)).detach().numpy()
    preds = [1 if i>0.5 else 0 for i in probs] # TODO: is this the best approach?
    return preds, probs


def get_loss_plot(train_losses, test_losses, 
                  train_label="Train loss", test_label="Validation loss"):
    # Sample data
    epochs = range(1, len(train_losses)+1)

    # Plotting the lines
    plt.plot(epochs, train_losses, label=train_label)
    plt.plot(epochs, test_losses, label=test_label)

    # Adding labels and title
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Model Loss')

    # Adding a legend
    plt.legend()
    return plt
    

def get_performance_metrics(model, train_dataset, test_dataset, config):
    
    x_train = train_dataset.x
    additional_train = train_dataset.additional
    y_train = train_dataset.y
    x_test = test_dataset.x
    additional_test = test_dataset.additional
    y_test = test_dataset.y
    

    logging.info("# making predictions on train and test sets")  
    logging.info("computing model predictions on train set")
    y_train_preds, y_train_probas = get_pnet_preds_and_probs(train_dataset, model)
    plt.hist(y_train_preds)

    logging.info("computing model predictions on test set")
    y_test_preds, y_test_probas = get_pnet_preds_and_probs(test_dataset, model)
    plt.hist(y_test_preds)


    logging.info("# calculating and logging useful performance metrics")
    train_score = accuracy_score(y_train, y_train_preds, normalize=True)
    test_score = accuracy_score(y_test, y_test_preds, normalize=True)
    wandb.run.summary["train_score"] = train_score # we use wandb.run.summary instead of wandb.log when we don't want multiple time steps
    wandb.run.summary["test_score"] = test_score
    wandb.run.summary["test_roc_auc_score"] = roc_auc_score(y_test, y_test_probas) # expects true_labels, predicted_probs
    wandb.run.summary['train_balanced_acc'] = balanced_accuracy_score(y_train, y_train_preds)
    wandb.run.summary['test_balanced_acc'] = balanced_accuracy_score(y_test, y_test_preds)
    
    logging.info(f"train score: {train_score}, test_score: {test_score}")
    logging.info(confusion_matrix(y_test, y_test_preds))
    
    wandb.log({
        "train_confusion_matrix": plot_confusion_matrix(y_train, y_train_preds), # expects true_labels, predicted_labels
        "test_confusion_matrix": confusion_matrix(y_test, y_test_preds),
        "test_classification_report": classification_report(y_test, y_test_preds) # expects true_labels, predicted_labels
              })
    
    wandb.sklearn.plot_confusion_matrix(y_test, y_test_preds)
    wandb.sklearn.plot_summary_metrics(model, x_train, y_train, x_test, y_test)
    
    # TODO: delete the stuff below here; I think it is redundant
    logging.info("train set metrics")
    cm_train = confusion_matrix(y_train, y_train_preds)
    print(classification_report(y_train, y_train_preds))
    print(roc_auc_score(y_train, y_train_probas))
    print(cm_train)

    logging.info("validation set metrics")
    cm_test = confusion_matrix(y_test, y_test_preds) # can also do (test_preds >= 0.5) instead of the list comprehension
    print(classification_report(y_test, y_test_preds)) 
    print(roc_auc_score(y_test, y_test_probas))
    print(cm_test)

    logging.info("# returning the predictions and prediction probabilities for train/test sets")
    return y_train_preds, y_test_preds, y_train_probas, y_test_probas


def report_df_info(*dataframes, n=5): # TODO: use this instead of load_df_verbose
    """
    Report information about an arbitrary number of dataframes.

    Parameters:
    *dataframes (pd.DataFrame): Arbitrary number of dataframes to report information about.
    n (int): Number of columns and indices to display.

    Returns:
    None

    # Example usage:
    data1 = {'A': [1, 2, 3], 'B': [4, 5, 6]}
    data2 = {'X': [7, 8, 9], 'Y': [10, 11, 12]}

    df1 = pd.DataFrame(data1, index=['row1', 'row2', 'row3'])
    df2 = pd.DataFrame(data2, index=['row4', 'row5', 'row6'])

    # Call the function to report information about the dataframes
    report_df_info(df1, df2)
    """

    for idx, df in enumerate(dataframes, start=1):
        print(f"----- DataFrame {idx} Info -----")
        print(f"Shape: {df.shape}")
        print(f"First {n} columns: {df.columns[:n].tolist()}")
        print(f"First {n} indices: {df.index[:n].tolist()}")
        print("-----")
    return


def report_df_info_with_names(df_dict, n=5):
    """
    Report information about dataframes (with DF names provided in a dictionary for convenience).

    Parameters:
    df_dict (dict): A dictionary where keys are names and values are dataframes.
    n (int): Number of columns and indices to display.

    Returns:
    None

    # Example usage:
    data1 = {'A': [1, 2, 3], 'B': [4, 5, 6]}
    data2 = {'X': [7, 8, 9], 'Y': [10, 11, 12]}

    df1 = pd.DataFrame(data1, index=['row1', 'row2', 'row3'])
    df2 = pd.DataFrame(data2, index=['row4', 'row5', 'row6'])

    # Create a dictionary with dataframe names
    dataframes_dict = {"DataFrame 1": df1, "DataFrame 2": df2}

    # Call the function with the dictionary of dataframes
    report_df_info_with_names(dataframes_dict)

    # Alternatively, use dict(zip()) instead of writing out a dictionary
    names = ['Dataframe 1', 'DF2']
    dfs = [df1, df2]
    report_df_info_with_names(dict(zip(names, dfs)))
    """

    for name, df in df_dict.items():
        print(f"----- DataFrame {name} Info -----")
        print(f"Shape: {df.shape}")
        print(f"First {n} columns: {df.columns[:n].tolist()}")
        print(f"First {n} indices: {df.index[:n].tolist()}")
        print("-----")
    return


def main():
    wandb.login()
    """# Load each of your data modalities of interest.
    Format should be samples x genes. Set the sample IDs as the index.

    Data modalities:
    1. somatic amp
    1. somatic del
    1. somatic mut
    1. germline mut (subset to a small number of genes).

    Our somatic data has information for many more genes compared to the germline data. We will need to either:
    1. impute zeros for the excluded germline genes, or 
    2. subset the somatic datasets down to the ones that overlap with the germline data.


    We will be subsetting to the ~1060 samples that we have matched somatic and germline data for.

    NOTE: the PNET loader will automatically restrict the input data modalities to overlapping samples and overlapping genes. 
    This means we will need to be carefull with our input germline dataset if we want to keep all the somatic data."""
    
    # TODO: eventually, want this overall structure of hyperparameters and function calls
    USE_ONLY_PAIRED = True
    CONVERT_IDS_TO = "somatic"
    ZERO_IMPUTE_GERMLINE = True
    ZERO_IMPUTE_SOMATIC = False

    logging.debug("Defining paths for somatic data")
    SOMATIC_DATADIR = "../../pnet_germline/data/pnet_database/prostate/processed"
    somatic_mut_f = os.path.join(SOMATIC_DATADIR, "P1000_final_analysis_set_cross_important_only.csv")
    somatic_cnv_f = os.path.join(SOMATIC_DATADIR, "P1000_data_CNA_paper.csv") # TODO: check how PNET implements splitting this into amp vs del
    
    logging.debug("Defining paths for germline data")
    GERMLINE_DATADIR = "../../pnet_germline/data/"
    # germline_vars_f = os.path.join(GERMLINE_DATADIR, "prostate/prostate_germline_vcf_subset_to_germline_tier_1and2_pathogenic_vars_only.txt")
    germline_vars_f = os.path.join(GERMLINE_DATADIR, "prostate/prostate_germline_vcf_subset_to_germline_tier_12_and_somatic_pathogenic_vars_only.txt")

    logging.debug("Defining paths for the sample metadata")
    id_map_f = os.path.join(GERMLINE_DATADIR, "prostate/germline_somatic_id_map_outer_join.csv") # germline_somatic_id_map_f
    sample_metadata_f = os.path.join(GERMLINE_DATADIR,"prostate/pathogenic_variants_with_clinical_annotation_1341_aug2021_correlation.csv")

    somatic_mut = get_somatic_mutation(somatic_mut_f)
    somatic_amp, somatic_del = get_somatic_amp_and_del(somatic_cnv_f)
    germline_mut = get_germline_mutation(germline_vars_f)  
    y = get_target(id_map_f, sample_metadata_f, id_to_use="Tumor_Sample_Barcode", target_col="is_met")
    # TODO: deal with adding confounder dataset: get_confounders(), etc.

    if USE_ONLY_PAIRED or CONVERT_IDS_TO: # Need to test the function going both ways, to somatic and to germline
        harmonize_prostate_ids(datasets_w_germline_ids=[germline_mut], 
                               datasets_w_somatic_ids=[somatic_mut, somatic_amp, somatic_del, y], 
                               convert_ids_to=CONVERT_IDS_TO) # want to run stuff if change_to_somatic_ids or paired
    if USE_ONLY_PAIRED: # want to run stuff if paired; note that this will only work correctly if the IDs are correctly harmonized
        logging.info("Restrict to overlapping samples (the indices)")
        somatic_mut, somatic_amp, somatic_del, germline_mut, y = restrict_to_overlapping_indices(somatic_mut, somatic_amp, somatic_del, 
                                                                                                 germline_mut, 
                                                                                                 y)

    # zero impute dataset columns (genes) as necessary (maybe have a reference dataset vs all others? Unsure of best way to parameterize this function)
    germline_mut = zero_impute_germline_datasets(germline_datasets=[germline_mut], 
                                  somatic_datasets=[somatic_mut, somatic_amp, somatic_del], 
                                  zero_impute_germline=ZERO_IMPUTE_GERMLINE) 
    somatic_mut, somatic_amp, somatic_del = zero_impute_somatic_datasets(germline_datasets=[germline_mut], 
                                 somatic_datasets=[somatic_mut, somatic_amp, somatic_del], 
                                 zero_impute_somatic=ZERO_IMPUTE_SOMATIC)


    logging.info("Now that we have processed all our datasets, we restrict to just the overlapping genes (the columns) (and further filter to those that fit some TCGA criteria)")
    somatic_mut, somatic_amp, somatic_del, germline_mut = restrict_to_genes_in_common(somatic_mut, somatic_amp, somatic_del, germline_mut)
    
    
    # TODO: change so that the function call is inside a logging statement.
    df_dict = dict(zip(
        ['somatic_mut', 'somatic_amp', 'somatic_del', 'germline_mut', 'y'], 
        [somatic_mut, somatic_amp, somatic_del, germline_mut, y]
    ))
    report_df_info_with_names(df_dict, n=5)


    logging.info("Finished preparing data format. Now left with: making train/val/test splits, setting up model, running model, evaluate model")

    ####------ below here, working on slowly getting reworking the code so we fit into the above function calls

    # TODO: temporarily include this information so we can directly compare to previous implementation
    logging.info("restricting metadata to samples with both germline and somatic samples")
    sample_metadata = sample_metadata.dropna(subset=["vcf_germline_id", "Tumor_Sample_Barcode"])
    logging.info(f"We have {len(sample_metadata)} pairs between the germline and somatic data")
    logging.info(f"These are the {len([i for i in somatic_mut.index.tolist() if i not in sample_metadata.Tumor_Sample_Barcode.tolist()])} unpaired samples")
    logging.debug(f"samples: {[i for i in somatic_mut.index.tolist() if i not in sample_metadata.Tumor_Sample_Barcode.tolist()]}")

    
    logging.info("# Generate the PNET loader")
    PNET_SPLITS_DIR = "../../pnet_germline/data/pnet_database/prostate/splits"
    TRAIN_SET_INDS_F = os.path.join(PNET_SPLITS_DIR, "training_set.csv")
    VALIDATION_SET_INDS_F = os.path.join(PNET_SPLITS_DIR, "validation_set.csv")
    DATASETS_TO_USE = ['somatic_amp', 'somatic_del', 'somatic_mut', 'germline_mut']
    genetic_data = {
                    'somatic_amp': somatic_amp, 
                    'somatic_del': somatic_del,
                    'somatic_mut': somatic_mut,
                    'germline_mut': germline_mut
                   }

    genetic_data = {key: genetic_data[key] for key in DATASETS_TO_USE if key in genetic_data}
    logging.info(f"Dictionary keys of datasets we will use: {genetic_data.keys()}")

    training_inds = pd.read_csv(TRAIN_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    validation_inds = pd.read_csv(VALIDATION_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    # test_inds = pd.read_csv(os.path.join(PNET_SPLITS_DIR, "test_set.csv"), usecols=["id","response"], index_col="id").index.tolist()
                        

    logging.info("# Train with run()")
    logging.info("P-NET PAPER SPLIT: what are the results using the same data split as in the P-NET paper, albeit with fewer samples?")
    train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, y,
                                                                 train_inds=training_inds,
                                                                 test_inds=validation_inds)

    # logging.info("POSITIVE CONTROL: can we overfit to the training data if we set many of the feature columns equal to the target column?")
    # train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, y,
    #                                                              add_N_perfectly_correlated_with_target=50,
    #                                                              train_inds=training_inds,
    #                                                              test_inds=validation_inds
    #                                                              )

    logging.info("Build the Reactome network structure")
    reactome_network = ReactomeNetwork.ReactomeNetwork(train_dataset.get_genes())

    logging.info("Define the hyperparameters of your modeling run")
    hparams={
        'reactome_network':reactome_network, 
        'nbr_gene_inputs':len(genetic_data), 
        'dropout':0.2, 
        'additional_dims':0, 
        'output_dim':1,
        'lr':1e-3, 
        'weight_decay':1e-5,
        'epochs':1000,
        'early_stopping':False,
        'batch_size':256,
        'verbose':True,
        'zero_impute_germline':ZERO_IMPUTE_GERMLINE,
        'zero_impute_somatic':ZERO_IMPUTE_SOMATIC,    
        'train_set_indices_f':TRAIN_SET_INDS_F,
        'validation_set_indices_f':VALIDATION_SET_INDS_F,
        'train_set_indices':training_inds,
        'validation_set_indices':validation_inds,
        'restricted_to_pairs':USE_ONLY_PAIRED
    }
    
    run = wandb.init(
        # Set the project where this run will be logged
        project="prostate_met_status",
        name="pnet_germline_somatic_paired",
        # Track hyperparameters and run metadata
        config=hparams
    )

    logging.info("make data loaders")
    train_loader, val_loader = pnet_loader.to_dataloader(train_dataset, test_dataset, hparams['batch_size'])

    logging.info(f"run PNET")
    model = Pnet.PNET_NN(reactome_network=hparams['reactome_network'], 
                         nbr_gene_inputs=hparams['nbr_gene_inputs'], 
                         output_dim=hparams['output_dim'],
                         dropout=hparams['dropout'],
                         additional_dims=hparams['additional_dims'],
                         lr=hparams['lr'], 
                         weight_decay=hparams['weight_decay'])

    
    logging.info("checking model convergence by examining a plot of how the loss changes over time")
    #for testing mutation not loss [Gwen: what does this mean?]
    model, train_losses, test_losses = Pnet.train(model, train_loader, val_loader, epochs=hparams['epochs'], verbose=hparams['verbose'], early_stopping=hparams['early_stopping'])
    plt = get_loss_plot(train_losses=train_losses, test_losses=test_losses)
    plt.show()

    logging.info("checking model performance")
    logging.info("Checking performance metrics for train/test")
    y_train_preds, y_test_preds, y_train_probas, y_test_probas = get_performance_metrics(model, train_dataset, test_dataset, config=hparams)

    train_preds = torch.sigmoid(model(train_dataset.x, train_dataset.additional)).detach().numpy()
    plt.hist(train_preds)

    test_preds = torch.sigmoid(model(test_dataset.x, test_dataset.additional)).detach().numpy()
    plt.hist(test_preds)

    print("train set metrics")
    cm_train = confusion_matrix(train_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in train_preds])
    print(classification_report(train_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in train_preds]))
    print(roc_auc_score(train_dataset.y.detach().numpy(), train_preds))

    print(cm_train)

    print("\nvalidation set metrics")
    cm_test = confusion_matrix(test_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in test_preds]) # can also do (test_preds >= 0.5) instead of the list comprehension
    print(classification_report(test_dataset.y.detach().numpy(), [1 if i>0.5 else 0 for i in test_preds])) 
    print(roc_auc_score(test_dataset.y.detach().numpy(), test_preds))
    print(cm_test)
    
    logging.info("ending wandb run")
    wandb.finish()
    return

    
if __name__=="__main__":
    main()