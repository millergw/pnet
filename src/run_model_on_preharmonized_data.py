# Script to run germline:somatic experiment P-NET on pre-formatted prostate germline and/or somatic data
# Author: Gwen Miller
# Related to the `run_pnet.py` script, which was generated by copying over and then editing everything in the file: `pnet/notebooks/prostate_metastatic_prediction_example.ipynb`

import os
import pandas as pd
import torch
import yaml
import configargparse
import wandb

import pnet_loader
import Pnet

# Gwen's scripts
import report_and_eval
import model_selection

import logging
logging.basicConfig(
            filename='run_pnet.log', 
            encoding='utf-8',
            format='%(asctime)s %(levelname)-8s %(message)s',
            level=logging.INFO,
            datefmt='%Y-%m-%d %H:%M:%S')

logger = logging.getLogger()
logger.setLevel(logging.INFO)

def parse_arguments():
    parser = configargparse.ArgumentParser(description="Description of your script")
    parser.add("--config_f", type=str, required=False, is_config_file=True, help="Path to a config file")
    parser.add("--data_config_f", type=str, required=True, is_config_file=False, help="Path to a config file")
    parser.add("--datasets", nargs="+", default=["somatic_amp", "somatic_del", "somatic_mut"],
                        help="List of datasets to use")
    parser.add("--evaluation_set", default="validation", choices=["validation", "test"],
                        help="Evaluation set (validation or test)")
    parser.add("--model_type", default="bdt", choices=['bdt', 'rf', 'pnet'],
                        help="Type of model")
    parser.add("--wandb_group", default="", help="Wandb group name")
    parser.add("--wandb_project", default="prostate_met_status", help="Wandb group name")
    parser.add("--seed", type=int, default=123, help="Seed value")
    parser.add("--input_data_dir", help="Directory with model-ready data")
    parser.add("--data_split_dir", default="../../pnet_germline/data/pnet_database/prostate/splits", 
               help="Directory with data split files")
    parser.add("--input_data_wandb_id", default="", 
               help="W&B run ID that created the data in the input_data_dir, if applicable")
    # parser.add('--genetic_data', type=yaml.load, help="TODO: dictionary-style yaml")
    return parser.parse_args()


def read_config(filename):
    with open(filename, 'r') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """# Load each of your data modalities of interest.
    Format should be samples x genes. Set the sample IDs as the index.

    Data modalities:
    1. somatic amp
    1. somatic del
    1. somatic mut
    1. germline mut (subset to a small number of genes).


    NOTE: the PNET loader will automatically restrict the input data modalities to overlapping samples and overlapping genes. 
    This means we will need to be carefull with our input germline dataset if we want to keep all the somatic data.
    """

    logging.debug("Parsing command-line arguments")
    args = parse_arguments()
    
    WANDB_GROUP = args.wandb_group
    wandb.login()
    wandb.init(
        # Set the project where this run will be logged
        project=args.wandb_project,
        group=WANDB_GROUP
    )
    wandb_run_id = wandb.run.id

    # Access the values
    DATASETS_TO_USE = args.datasets
    EVALUATION_SET = args.evaluation_set
    MODEL_TYPE = args.model_type
    SEED = args.seed
    MODEL_TYPE = args.model_type
    EVALUATION_SET = args.evaluation_set
    SPLITS_DIR = args.data_split_dir
    TRAIN_SET_INDS_F = os.path.join(SPLITS_DIR, "training_set.csv")
    EVALUATION_SET_INDS_F = os.path.join(SPLITS_DIR, f"{EVALUATION_SET}_set.csv")

    Pnet.set_random_seeds(SEED, turn_off_cuDNN=False)

    # TODO: add save dir
    SAVE_DIR = f"../results/{MODEL_TYPE}_eval_set_{EVALUATION_SET}/wandbID_{wandb.run.id}"
    if WANDB_GROUP != "":
        SAVE_DIR = f"../results/{WANDB_GROUP}/{MODEL_TYPE}_eval_set_{EVALUATION_SET}/wandbID_{wandb.run.id}"
    report_and_eval.make_dir_if_needed(SAVE_DIR)

    # TODO: need to figure out how to read in the dictionary style items (all my data)
    config = read_config(args.data_config_f)

    logging.info("Loading data from directory {}".format(args.input_data_dir))
    input_data_wandb_id = args.input_data_wandb_id

    for name, info in config['genetic_data'].items():
        config['genetic_data'][name]['df'] = pd.read_csv(os.path.join(args.input_data_dir, info['filename']), index_col=0) # NOTE: setting the first column as index

    config['confounder_data']['df'] = pd.read_csv(os.path.join(args.input_data_dir, config['confounder_data']['filename']), index_col=0)
    config['target']['df'] = pd.read_csv(os.path.join(args.input_data_dir, config['target']['filename']), index_col=0)

    additional = config['confounder_data']['df']
    y = config['target']['df']

    # Pnet class expects to be passed a dict in format {name1:df1, name2:df2}
    genetic_data = {}
    for name, info in config['genetic_data'].items():
        genetic_data[name] = info['df']
        logging.debug(info['df'].shape)

    genetic_data = {key: genetic_data[key] for key in DATASETS_TO_USE if key in genetic_data}
    logging.info(f"Dictionary keys of datasets we will use: {genetic_data.keys()}")

    logging.info("Reporting summary information about all the input data.")
    report_and_eval.report_df_info_with_names(genetic_data, n=5)
    report_and_eval.report_df_info_with_names({'additional':additional,
                                               'y':y}, n=5)

    training_inds = pd.read_csv(TRAIN_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()
    evaluation_inds = pd.read_csv(EVALUATION_SET_INDS_F, usecols=["id","response"], index_col="id").index.tolist()

    logging.info("Defining the hyperparameters of the modeling run")
    hparams={
        'wandb_run_id_that_created_inputs':input_data_wandb_id,
        'nbr_gene_inputs':len(genetic_data), 
        'dropout':0.2, 
        'additional_dims':0, 
        'output_dim':1,
        'lr':1e-3, 
        'weight_decay':1e-3,
        'epochs':500, # 500, 3 if testing something
        'early_stopping':True,
        'batch_size':64,
        'verbose':True,   
        'train_set_indices_f':TRAIN_SET_INDS_F,
        'evaluation_set_indices_f':EVALUATION_SET_INDS_F,
        'train_set_indices':training_inds,
        'evaluation_set_indices':evaluation_inds,
        'dataset':list(genetic_data.keys()),
        'random_seed':SEED,
        'model_type':MODEL_TYPE,
        'evaluation_set': EVALUATION_SET,
        'save_dir': SAVE_DIR,
    }
    
    logging.info("Adding hyperparameters and run metadata to Weights and Biases")
    wandb.config.update(hparams)
    if MODEL_TYPE in ['rf', 'bdt']:
        logging.info("Loading data and making data splits")
        train_dataset, test_dataset = pnet_loader.generate_train_test(genetic_data, additional_data=additional, target=y, train_inds=training_inds, test_inds=evaluation_inds, gene_set=None)
        logging.info("Merging the genetic data and additional data (e.g. confounders). Updating the input_df and x attributes.")
        train_dataset.input_df = pd.concat([train_dataset.input_df, train_dataset.additional_data], axis=1)
        train_dataset.x = torch.cat([train_dataset.x, train_dataset.additional], dim=1)
        test_dataset.input_df = pd.concat([test_dataset.input_df, test_dataset.additional_data], axis=1)
        test_dataset.x = torch.cat([test_dataset.x, test_dataset.additional], dim=1)

        x_train = train_dataset.x
        y_train = train_dataset.y.ravel()
        x_test = test_dataset.x
        y_test = test_dataset.y.ravel()

    if MODEL_TYPE == 'rf':
        model = model_selection.run_rf(x_train, y_train, random_seed=None)

    elif MODEL_TYPE == 'bdt':
        model = model_selection.run_bdt(x_train, y_train, random_seed=None)
        # TODO: start here 2/15. Unsure if test_dataset.input or test_dataset.x is appropriate for the get_deviance function.
        logging.info(f"Making deviance plots to check convergence/overfitting for model")
        train_scores, test_scores = report_and_eval.get_deviance(model, x_test, y_test)
        plt = report_and_eval.get_loss_plot(train_losses=train_scores, test_losses=test_scores,
                                            train_label="Train deviance", test_label=f"{EVALUATION_SET} deviance",
                                            title="Model Deviance", ylabel="Deviance (MSE)", xlabel="Boosting iterations")
        report_and_eval.savefig(plt, os.path.join(SAVE_DIR, f'deviance_per_boosting_iteration'))
        wandb.log({"convergence plot": plt})
        plt.show()

    if MODEL_TYPE == "pnet":
        logging.info("Train with Pnet.run()")
        model, train_losses, test_losses, train_dataset, test_dataset = Pnet.run(genetic_data, y, 
                                                                                save_path=os.path.join(SAVE_DIR, "model.pt"), # TODO: this has suddenly stopped working. I think the only big change was getting a different version of CUDA to match my version of PyTorch..
                gene_set=None, additional_data=additional, # TODO: need some way of tracking which additional features are used. Maybe can extract and save with W&B? 
                dropout=hparams['dropout'], input_dropout=0.5, lr=hparams['lr'], 
                weight_decay=hparams['weight_decay'], batch_size=hparams['batch_size'], epochs=hparams['epochs'], 
                verbose=hparams['verbose'], early_stopping=hparams['early_stopping'], 
                train_inds=hparams['train_set_indices'], test_inds=hparams['evaluation_set_indices'], 
                random_network=False, fcnn=False, task=None, loss_fn=None, loss_weight=None, aux_loss_weights=[2, 7, 20, 54, 148, 400])
    
        logging.info("Check model convergence by examining the plot of how loss changes over time")
        plt = report_and_eval.get_loss_plot(train_losses=train_losses, test_losses=test_losses)
        report_and_eval.savefig(plt, os.path.join(SAVE_DIR, 'loss_over_time'))
        # instead of doing plt.show() do: # see https://docs.wandb.ai/guides/integrations/scikit
        wandb.log({"convergence plot": plt})
        plt.show()

    logging.info(f"Get the model predictions, performance metrics, feature importances, and save the results to {SAVE_DIR}.")
    report_and_eval.evaluate_interpret_save(model=model, pnet_dataset=train_dataset, model_type=MODEL_TYPE, who="train", save_dir=SAVE_DIR)
    report_and_eval.evaluate_interpret_save(model=model, pnet_dataset=test_dataset, model_type=MODEL_TYPE, who=EVALUATION_SET, save_dir=SAVE_DIR)
    
    logging.info("ending wandb run")
    wandb.finish()
    return wandb_run_id


if __name__=="__main__":
    main()